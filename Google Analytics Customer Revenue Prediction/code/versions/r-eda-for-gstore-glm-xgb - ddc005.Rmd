---
title: "Google Analytics Customer Revenue Prediction EDA"
output:
  html_document:
    code_folding: show
    fig_caption: yes
    fig_height: 4.5
    fig_width: 7.5
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

# Introduction

Here is an Exploratory Data Analysis for the Google Analytics Customer Revenue Prediction competition 
within the R environment. For this EDA in the main we will use [tidyverse](https://www.tidyverse.org/packages/) packages. 
Also for modelling we will use [glmnet](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html), 
[xgboost](https://cran.r-project.org/web/packages/xgboost/vignettes/xgboostPresentation.html) and 
[keras](https://keras.rstudio.com/) packages.

Our task is to build an algorithm that predicts the natural log of the sum of all transactions per user. Thus, 
for every user in the test set, the target is:

$$y_{user} = \sum_{i=1}^n transaction_{user_i}$$

$$target_{user} = ln(y_{user}+1).$$

Submissions are scored on the root mean squared error, which is defined as:

$$RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \widehat{y_i})^2},$$

where $\widehat{y}$ is the predicted revenue for a customer and $y$ is the natural log of the actual revenue value.

Let's prepare and have a look at the dataset.

# Preparations {.tabset .tabset-fade .tabset-pills}
## Load libraries
Here we load libraries for data wrangling and visualisation.
```{r, message=FALSE, warning=FALSE, results='hide'}
library(h2o)
library(caret)
library(lme4)
library(ggalluvial)
library(xgboost)
library(jsonlite)
library(lubridate)
library(knitr)
library(Rmisc)
library(scales)
library(countrycode)
library(highcharter)
library(glmnet)
library(keras)
library(forecast)
library(zoo)
library(magrittr)
library(tidyverse)
library(stringr)
library(forcats)
```

## Load data
```{r load, message=FALSE, warning=FALSE, results='hide'}
set.seed(0)

tr <- read_csv("../input/train_s.csv")
te <- read_csv("../input/test_s.csv")
subm <- read_csv("../input/sample_submission.csv")
```

# Peek at the dataset 
## General info
```{r info, result='asis', echo=FALSE}
cat("Train set file size:", file.size("../input/train_s.csv"), "bytes")
cat("Train set dimensions:", dim(tr))
glimpse(tr)
cat("\n")
cat("Test set file size:", file.size("../input/test_s.csv"), "bytes")
cat("Test set dimensions:", dim(te))
glimpse(te)
```

## Distribution of transaction dates
As shown in the figure, there are only a few of the transactions after Jan 2018 in the train set, 
because the rest is in the test set. It makes sense to create time-based splits for train/validation sets.

```{r dates_distr, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
p1 <- tr %>% mutate(date = ymd(date), 
                    year_month = make_date(year(date), month(date))) %>% 
  group_by(year_month) %>% count() %>% 
  ggplot(aes(x = year_month, y = n)) +
  geom_bar(stat="identity", fill="steelblue") +
  labs(x = "", y = "transactions", title = "Train") +
  theme_minimal() +
  scale_x_date(labels = date_format("%Y - %m"))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_vline(aes(xintercept = max(year_month), colour = "red"), size = 1) +
  theme(legend.position="none")
p2 <- te %>% mutate(date = ymd(date), 
                    year_month = make_date(year(date), month(date))) %>% 
  group_by(year_month) %>% count() %>% 
  ggplot(aes(x = year_month, y = n)) +
  geom_bar(stat="identity", fill="steelblue") +
  labs(x = "", y = "transactions",  title = "Test") +
  theme_minimal() +
  scale_x_date(labels = date_format("%Y - %m"))+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))    
multiplot(p1, p2, cols = 2)
```

## Dataset columns
There is a total of 13 features: 

* **channelGrouping** - the channel via which the user came to the Store
* **customDimensions** - customer profile
* **date** - the date on which the user visited the Store
* **device** - the specifications for the device used to access the Store
* **fullVisitorId** - an unique identifier for each user of the Google Merchandise Store
* **geoNetwork** - this section contains information about the geography of the user
* **hits** - user actions during the session
* **socialEngagementType** - engagement type, either "Socially Engaged" or "Not Socially Engaged"
* **totals** - this section contains aggregate values across the session
* **trafficSource** - this section contains information about the Traffic Source from which the session originated
* **visitId** - an identifier for this session
* **visitNumber** - the session number for this user
* **visitStartTime** - the timestamp (POSIX).

Let's have a look at counts of the simple features:
```{r counts, result='asis',  warning=FALSE, echo=FALSE}
tr %>% select(fullVisitorId, channelGrouping, date, 
              socialEngagementType, visitId, 
              visitNumber, visitStartTime) %>% 
  map_dfr(n_distinct) %>% 
  gather() %>% 
  ggplot(aes(reorder(key, -value), value)) +
  geom_bar(stat = "identity", fill="steelblue") + 
  scale_y_log10(breaks = c(5, 50, 250, 500, 1000, 10000, 50000)) +
  geom_text(aes(label = value), vjust = 1.6, color = "white", size=3.5) +
  theme_minimal() +
  labs(x = "features", y = "Number of unique values") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

At least one column can be removed.

## JSON data
Actually the columns **device**, **geoNetwork**, **trafficSource**, **totals**, **customDimensions**, **hits** contain data in JSON format. 
To parse it we can use [jsonlite](https://cran.r-project.org/web/packages/jsonlite/) package but due to inconsistency 
of the data form the **trafficSource** column we need some addional tricks. I suggest to use the code, 
which is based on [this idea](https://www.kaggle.com/mrlong/r-flatten-json-columns-to-make-single-data-frame):
```{r fun0, message=FALSE, warning=FALSE, results='hide'}
flatten_json <- . %>% 
  str_c(., collapse = ",") %>% 
  str_c("[", ., "]") %>% 
  fromJSON(flatten = T)

parse <- . %>% 
  bind_cols(flatten_json(.$customDimensions[0])) %>% 
  bind_cols(flatten_json(.$hits[0])) %>% 
  bind_cols(flatten_json(.$device)) %>%
  bind_cols(flatten_json(.$geoNetwork)) %>% 
  bind_cols(flatten_json(.$trafficSource)) %>% 
  bind_cols(flatten_json(.$totals)) %>% 
  select(-customDimensions, -hits, -device, -geoNetwork, -trafficSource, -totals)
```
**str_c** function is a little faster than **paste()**.

Let's convert train and test sets to the tidy format:

```{r df_conv, message=FALSE, warning=FALSE, results='show'}
tr <- parse(tr)
te <- parse(te)
```

## Tidy datasets  {.tabset}
### Train
```{r, result='asis', echo=FALSE}
kable(head(tr, 2))
```

### Test
```{r, result='asis', echo=FALSE}
kable(head(te, 2))
```

### Sample Submission
```{r, result='asis', echo=FALSE}
kable(head(subm, 5))
```

## Train and test features sets intersection
```{r tr_te_fea_int, result='asis', echo=TRUE}
# setdiff(names(tr), names(te))
tr %<>% select(-one_of("transactions"))
tr %<>% select(-one_of("totalTransactionRevenue"))
te %<>% select(-one_of("transactions"))
te %<>% select(-one_of("totalTransactionRevenue"))
te %<>% select(-one_of("transactionRevenue"))

setdiff(names(tr), names(te))
```

The test set lacks one columns, which is a target variable **transactionRevenue**.


## Constant columns
Let's find constant columns: 
```{r del_f, result='asis', echo=TRUE}
fea_uniq_values <- sapply(tr, n_distinct)
(fea_del <- names(fea_uniq_values[fea_uniq_values == 1]))

tr %<>% select(-one_of(fea_del))
te %<>% select(-one_of(fea_del))
```
All these useless features we can safely remove.

## Expanded train features
```{r tr_te_fea_expand, result='asis', echo=TRUE}
names(tr)
# names(te)
```

## Missing values
After parsing of the JSON data we can observe many missing values in the data set.
Let's find out how many missing values each feature has. We need to take into account that 
such values as "not available in demo dataset", "(not set)", "unknown.unknown", "(not provided)" 
can be treated as NA.
```{r nas0, result='asis', echo=TRUE}
is_na_val <- function(x) x %in% c("not available in demo dataset", "(not provided)",
                                  "(not set)", "<NA>", "unknown.unknown",  "(none)")

tr %<>% mutate_all(list(~ifelse(is_na_val(.), NA, .)))
te %<>% mutate_all(list(~ifelse(is_na_val(.), NA, .)))
```
```{r nas1, result='asis', echo=FALSE}
tr %>% summarise_all(list(~sum(is.na(.))/n()*100)) %>% 
gather(key="feature", value="missing_pct") %>% 
  ggplot(aes(x=reorder(feature,-missing_pct),y=missing_pct)) +
  geom_bar(stat="identity", fill="steelblue")+
  labs(y = "missing %", x = "features") +
  coord_flip() +
  theme_minimal()
```

There is a bunch of features missing nearly completely. 

## data transformations
We need to convert some features to their natural representation.

```{r tf1, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
tr[0:3,]
```
```{r tf2, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
te[0:3,]
```

```{r tf3, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
tr %<>%
  mutate(date = ymd(date),
         hits = as.integer(hits1),
         pageviews = as.integer(pageviews),
         bounces = as.integer(bounces),
         newVisits = as.integer(newVisits),
         transactionRevenue = as.numeric(transactionRevenue))
tr %<>% select(-one_of("hits1"))
         
te %<>%
  mutate(date = ymd(date),
         hits = as.integer(hits1),
         pageviews = as.integer(pageviews),
         bounces = as.integer(bounces),
         newVisits = as.integer(newVisits)) 
te %<>% select(-one_of("hits1"))

``` 

## Target variable
As a target variable we use **transactionRevenue** which is a sub-column of the **totals** JSON column. It looks like
this variable is multiplied by $10^6$.

```{r target, result='asis', echo=TRUE}
y <- tr$transactionRevenue
tr$transactionRevenue <- NULL
summary(y)
```

We can safely replace **NA** values with 0.
```{r, result='asis', echo=TRUE}
y[is.na(y)] <- 0
summary(y)
```
```{r, result='asis', echo=FALSE}
p1 <- as_tibble(y) %>% 
  ggplot(aes(x = log1p(value))) +
  geom_histogram(bins = 30, fill="steelblue") + 
  labs(x = "transaction revenue") +
  theme_minimal()

p2 <- as_tibble(y[y>0]) %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill="steelblue") + 
  labs(x = "non-zero transaction revenue") +
  theme_minimal()

multiplot(p1, p2, cols = 2)

as_tibble(log1p(y[y>0] / 1e6)) %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, fill="steelblue") + 
  labs(x = "log(non-zero transaction revenue / 1e6)") +
  theme_minimal()
```

The target variable has a wide range of values. Its distribution is right-skewed. For modelling we will use
log-transformed target. 

BTW, only `r round(length(y[y!=0]) / length(y) * 100, 2)`% 
of all transactions have non-zero revenue:

```{r rev0, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  ggplot(aes(x = 1:length(value), y = value)) +
  geom_point(color = "steelblue",alpha=0.4, size=0.8) +
  theme_minimal() +
  scale_y_continuous(name="revenue", labels = comma) + 
  scale_x_continuous(name="index", labels = comma) +
  theme(legend.position="none")
```

The next figure shows that users who came via **Affiliates** and **Social**
channels do not generate revenue. The most profitable channel is **Referral**: 

```{r rev1, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  group_by(channelGrouping) %>% 
  summarise(revenue = sum(value)) %>%
  ggplot(aes(x = channelGrouping, y = revenue)) +
  geom_point(color="steelblue", size=2) +
  theme_minimal() +
  scale_y_continuous(labels = comma) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

Also usually first/less visit users generate more total revenue:

```{r rev2, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  group_by(visitNumber) %>% 
  summarise(revenue = sum(value)) %>%
  ggplot(aes(x = visitNumber, y = revenue)) +
  geom_point(color="steelblue", size=0.5) +
  theme_minimal() +
  scale_x_continuous(breaks=c(1, 3, 5, 10, 15, 25, 50, 100), limits=c(0, 105))+
  scale_y_continuous(labels = comma)

```

## How target variable changes in time
The revenue itself can be viewed as a timeseries. There seems to be a pattern of peaks.

```{r date, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
p1 <- tr %>% 
  bind_cols(as_tibble(y)) %>% 
  group_by(date) %>% 
  summarize(visits = n()) %>% 
  ungroup() %>% 
  ggplot(aes(x = date, y = visits)) + 
  geom_line() +
  geom_smooth() + 
  labs(x = "") +
  theme_minimal()

p2 <- tr %>% 
  bind_cols(as_tibble(y)) %>% 
  group_by(date) %>% 
  summarize(revenue = mean(value)) %>% 
  ungroup()  %>% 
  ggplot(aes(x = date, y = revenue)) + 
  geom_line() +
  stat_smooth() +
  labs(x = "") +
  theme_minimal()

multiplot(p1, p2, cols = 1)     
```

There is an interesting separation in target variable by **isTrueDirect** feature:
```{r target_time_isTD, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  group_by(date, isTrueDirect) %>% 
  summarize(revenue = mean(value)) %>% 
  ungroup()  %>% 
  ggplot(aes(x = date, y = revenue, color = isTrueDirect)) + 
  stat_smooth(aes(color = isTrueDirect)) +
  labs(x = "") +
  theme_minimal()
```


## Distribution of visits and revenue by attributes

```{r freq1, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  group_by(channelGrouping) %>% 
  summarize(visits = n(), mean_revenue = mean(value), total_revenue = sum(value)) %>% 
  ungroup() %>% 
  mutate(channelGrouping = reorder(channelGrouping, -visits)) %>% 
  data.table::melt(id.vars = c("channelGrouping")) %>% 
  ggplot(aes(channelGrouping, value, fill = variable)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap(~ variable, scales = "free") + 
  theme_minimal() +
  labs(x = "channel grouping", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="none") 
```

```{r freq2, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  mutate(browser = factor(browser) %>% fct_lump(prop=0.01)) %>% 
  group_by(browser) %>% 
  summarize(visits = n(), mean_revenue = mean(value), total_revenue = sum(value)) %>% 
  ungroup() %>% 
  mutate(browser = reorder(browser, -visits)) %>% 
  data.table::melt(id.vars = c("browser")) %>% 
  ggplot(aes(browser, value, fill = variable)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap(~ variable, scales = "free") + 
  theme_minimal() +
  labs(x = "browser", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="none") 
```

```{r freq3, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  mutate(operatingSystem = factor(operatingSystem) %>% fct_lump(prop=0.01)) %>% 
  group_by(operatingSystem) %>% 
  summarize(visits = n(), mean_revenue = mean(value), total_revenue = sum(value)) %>% 
  ungroup() %>% 
  mutate(operatingSystem = reorder(operatingSystem, -visits)) %>% 
  data.table::melt(id.vars = c("operatingSystem")) %>% 
  ggplot(aes(operatingSystem, value, fill = variable)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap(~ variable, scales = "free") + 
  theme_minimal() +
  labs(x = "operating system", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="none") 
```

```{r freq4, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  mutate(deviceCategory = factor(deviceCategory) %>% fct_lump(prop=0.01)) %>% 
  group_by(deviceCategory) %>% 
  summarize(visits = n(), mean_revenue = mean(value), total_revenue = sum(value)) %>% 
  ungroup() %>% 
  mutate(deviceCategory = reorder(deviceCategory, -visits)) %>% 
  data.table::melt(id.vars = c("deviceCategory")) %>% 
  ggplot(aes(deviceCategory, value, fill = variable)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap(~ variable, scales = "free") + 
  theme_minimal() +
  labs(x = "device category", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="none") 
```

```{r freq5, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  mutate(country = factor(country) %>% fct_lump(prop=0.023)) %>% 
  group_by(country) %>% 
  summarize(visits = n(), mean_revenue = mean(value), total_revenue = sum(value)) %>% 
  ungroup() %>% 
  mutate(country = reorder(country, -visits)) %>% 
  data.table::melt(id.vars = c("country")) %>% 
  ggplot(aes(country, value, fill = variable)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap(~ variable, scales = "free") + 
  theme_minimal() +
  labs(x = "country", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="none") 
```   

```{r freq6, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  mutate(city = factor(city) %>% fct_lump(prop=0.01)) %>% 
  group_by(city) %>% 
  summarize(visits = n(), mean_revenue = mean(value), total_revenue = sum(value)) %>% 
  ungroup() %>% 
  mutate(city = fct_explicit_na(city, na_level = "Other") %>% reorder(-visits)) %>% 
  data.table::melt(id.vars = c("city")) %>% 
  ggplot(aes(city, value, fill = variable)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap(~ variable, scales = "free") + 
  theme_minimal() +
  labs(x = "city", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="none") 
```   

```{r freq7, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  mutate(networkDomain = factor(networkDomain) %>% fct_lump(prop=0.01)) %>% 
  group_by(networkDomain) %>% 
  summarize(visits = n(), mean_revenue = mean(value), total_revenue = sum(value)) %>% 
  ungroup() %>% 
  mutate(networkDomain = fct_explicit_na(networkDomain, na_level = "Other") %>% reorder(-visits)) %>% 
  data.table::melt(id.vars = c("networkDomain")) %>% 
  ggplot(aes(networkDomain, value, fill = variable)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap(~ variable, scales = "free") + 
  theme_minimal() +
  labs(x = "network domain", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="none") 
```  

```{r freq8, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  bind_cols(as_tibble(y)) %>% 
  mutate(medium = factor(medium) %>% fct_lump(prop=0.005)) %>% 
  group_by(medium) %>% 
  summarize(visits = n(), mean_revenue = mean(value), total_revenue = sum(value)) %>% 
  ungroup() %>% 
  mutate(medium = fct_explicit_na(medium, na_level = "Other") %>% reorder(-visits)) %>% 
  data.table::melt(id.vars = c("medium")) %>% 
  ggplot(aes(medium, value, fill = variable)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = scales::comma) +
  facet_wrap(~ variable, scales = "free") + 
  theme_minimal() +
  labs(x = "medium", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position="none") 
```    


## Findings:

* The most frequent channels are **OrganicSearch** and **Social**.
* Chrome is the most popular browser and its users produce the highest total revenue.
* Windows and MacOS are the most popular operating systems. It's interesting that ChromeOS users yield the highest mean revenue.
* Desktops are still in the ranks.
* The US users yield the most of the total revenue.
* Usually netwok domain is unknown.
* **organic** and **referral** are the most popular mediums.

# Alluvial diagram
This kind of plot is useful for discovering of multi-feature interactions. 
The vertical size of each block is proportional to the frequency of the feature. 
This plot shows high flows from the US and UK desktops with Chrome.

```{r al1, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  select(country, networkDomain, browser, deviceCategory, channelGrouping) %>% 
  mutate(networkDomain = str_split(networkDomain, "\\.") %>% map(~ .x[[length(.x)]]) %>% unlist) %>% 
  mutate_all(factor) %>% 
  mutate_all(fct_lump, 4) %>% 
  bind_cols(tibble(revenue = ifelse(y == 0, "Zero", "Non-zero") %>% factor)) %>% 
  na.omit() %>% 
  group_by_all() %>% 
  count() %>% 
  ggplot(aes(y = n, 
             axis1 = country, axis2 = deviceCategory, axis3 = browser,   
             axis4 = channelGrouping, axis5 = networkDomain)) +
  geom_alluvium(aes(fill = revenue), width = 1/12) +
  geom_stratum(width = 1/10, fill = "black", color = "grey") +
  geom_label(stat = "stratum", label.strata = TRUE) +
  theme_minimal() +
  scale_x_continuous(breaks = 1:5, labels = c("country", "deviceCategory", "browser",
                                               "channelGrouping", "networkDomain"))
```

The next figure shows the flows for the case when revenue > 0:
```{r al2, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tr %>% 
  select(country, networkDomain, browser, deviceCategory, channelGrouping) %>% 
  mutate(networkDomain = str_split(networkDomain, "\\.") %>% map(~ .x[[length(.x)]]) %>% unlist) %>% 
  mutate_all(factor) %>% 
  mutate_all(fct_lump, 4) %>% 
  bind_cols(tibble(revenue = ifelse(y == 0, "Zero", "Non-zero") %>% factor)) %>% 
  na.omit() %>% 
  filter(revenue == "Non-zero") %>% 
  group_by_all() %>% 
  count() %>% 
  ggplot(aes(y = n, 
             axis1 = country, axis2 = deviceCategory, axis3 = browser,   
             axis4 = channelGrouping, axis5 = networkDomain)) +
  geom_alluvium(aes(fill = revenue), width = 1/12) +
  geom_stratum(width = 1/10, fill = "black", color = "grey") +
  geom_label(stat = "stratum", label.strata = TRUE) +
  theme_minimal() +
  scale_x_continuous(breaks = 1:5, labels = c("country", "deviceCategory", "browser",
                                               "channelGrouping", "networkDomain"))
```

Non-zero transaction revenue in the main is yielded by the flow 
US-desktop-Chrome-{OrganicSearch | Referral}-net.


# Correlations between revenue and features
Some features are categorical and we reencode them as OHE (with reduced set of levels). 
The ID columns are dropped.

```{r cor1, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
m <- tr %>% 
  mutate(year = year(date),
         month = month(date),
         day = day(date),
         isMobile = ifelse(isMobile, 1L, 0L),
         isTrueDirect = ifelse(isMobile, 1L, 0L)) %>% 
  mutate_all(funs(ifelse(is.na(.), 0, .))) %>% 
  select(-date, -fullVisitorId, -visitId) %>% 
  mutate_if(is.character, factor) %>% 
  mutate_if(is.factor, fct_lump, prop = 0.01) %>% 
  model.matrix(~ . - 1, .) %>% 
  cor(y) %>% 
  data.table::as.data.table(keep.rownames=TRUE) %>% 
  set_names("Feature", "rho") %>% 
  arrange(-rho) 

m %>% 
  ggplot(aes(x = rho)) +
  geom_histogram(bins = 50, fill="steelblue") + 
  labs(x = "correlation") +
  theme_minimal()
```

The values of the correlation coefficient are concentrated around zero, but there
are several values bigger than 0.3:
```{r cor2, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
m %>% 
  filter(rho > 0.3) %>% 
  kable()
```

Let’s visualize the relationship of the target variable with each of the correlated variables.

```{r cor3, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
p1 <- tr %>% 
  select(pageviews) %>% 
  bind_cols(as_tibble(y)) %>% 
  filter(value > 0) %>% 
  ggplot(aes(x = pageviews, y = log1p(value))) +
  geom_point() +
  labs(x = "pageviews", y = "transaction revenue") +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() 

p2 <- tr %>% 
  select(hits) %>% 
  bind_cols(as_tibble(y)) %>% 
  filter(value > 0) %>% 
  ggplot(aes(x = hits, y = log1p(value))) +
  geom_point() +
  labs(x = "hits", y = "transaction revenue") +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() 

multiplot(p1, p2, cols = 2)
```

Here we observe weak positive relationship. Although, these features can play important role in a statistical model.


# Revenue Predictive models 
It is always useful to create several analtical models and compare them. Here for all models we use a preprocessed dataset:

```{r bm1, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
grp_mean <- function(x, grp) ave(x, grp, FUN = function(x) mean(x, na.rm = TRUE))

idx <- tr$date < ymd("20171201")
id <- te[, "fullVisitorId"]
tri <- 1:nrow(tr)

tr_te <- tr %>%
  bind_rows(te) %>% 
  mutate(year = year(date) %>% factor(),
         wday = wday(date) %>% factor(),
         hour = hour(as_datetime(visitStartTime)) %>% factor(),
         isMobile = ifelse(isMobile, 1L, 0L),
         isTrueDirect = ifelse(isTrueDirect, 1L, 0L),
         adwordsClickInfo.isVideoAd = ifelse(!adwordsClickInfo.isVideoAd, 0L, 1L)) %>% 
  select(-date, -fullVisitorId, -visitId, -visitStartTime, -sessionQualityDim, -timeOnSite) %>% 
  mutate_if(is.character, factor) %>% 
  mutate(pageviews_mean_vn = grp_mean(pageviews, visitNumber),
         pageviews_mean_country = grp_mean(pageviews, country),
         pageviews_mean_city = grp_mean(pageviews, city),
         pageviews_mean_dom = grp_mean(pageviews, networkDomain),
         pageviews_mean_ref = grp_mean(pageviews, referralPath)) %T>% 
  glimpse()

# rm(tr, te, tr_ae, te_ae); invisible(gc())
```


## GLMNET - Generalized linear model
For the **glmnet** model we need a model matrix. We replace **NA** values with zeros, 
rare factor levels are lumped:
```{r glm1, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
tr_te_ohe <- tr_te %>% 
  mutate_if(is.factor, fct_explicit_na) %>% 
  mutate_if(is.numeric, funs(ifelse(is.na(.), 0L, .))) %>% 
  mutate_if(is.factor, fct_lump, prop = 0.05) %>% 
  select(-adwordsClickInfo.isVideoAd) %>% 
  model.matrix(~.-1, .) %>% 
  scale() %>% 
  round(4)

X <- tr_te_ohe[tri, ]
X_test <- tr_te_ohe[-tri, ]
rm(tr_te_ohe); invisible(gc())
```
The next step is to create a cross-validated LASSO model:
```{r glm2, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
m_glm <- cv.glmnet(X, log1p(y), alpha = 0, family="gaussian", 
                   type.measure = "mse", nfolds = 5)
```
Finally, we create predictions of the LASSO model
```{r glm3, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
pred_glm_tr <- predict(m_glm, X, s = "lambda.min") %>% c()
pred_glm <- predict(m_glm, X_test, s = "lambda.min") %>% c()
sub <- "glmnet_gs.csv"
# submit(pred_glm)
pred_glm[0:10]
rm(m_glm); invisible(gc())
```

## XGB - Gradient boosting decision trees
At last, we are ready to create an XGB model. First, 
we need to preprocess the dataset. We don't care about **NA** 
values - XGB handles them by default:

```{r xgb1, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
tr_te_xgb <- tr_te %>% 
  mutate_if(is.factor, as.integer) %>% 
  glimpse()
  
rm(tr_te); invisible(gc()) 
```

Second, we create train, validation and test sets. We use time-based split:
```{r xgb2, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
dtest <- xgb.DMatrix(data = data.matrix(tr_te_xgb[-tri, ]))
tr_te_xgb <- tr_te_xgb[tri, ]
dtr <- xgb.DMatrix(data = data.matrix(tr_te_xgb[idx, ]), label = log1p(y[idx]))
dval <- xgb.DMatrix(data = data.matrix(tr_te_xgb[!idx, ]), label = log1p(y[!idx]))
dtrain <- xgb.DMatrix(data = data.matrix(tr_te_xgb), label = log1p(y))
cols <- colnames(tr_te_xgb)
rm(tr_te_xgb); invisible(gc)
```
The next step is to train the model:
```{r xgb3, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
p <- list(objective = "reg:linear",
          booster = "gbtree",
          eval_metric = "rmse",
          nthread = 4,
          eta = 0.05,
          max_depth = 7,
          min_child_weight = 5,
          gamma = 0,
          subsample = 0.8,
          colsample_bytree = 0.7,
          colsample_bylevel = 0.6,
          nrounds = 2000)

set.seed(0)
m_xgb <- xgb.train(p, dtr, p$nrounds, list(val = dval), print_every_n = 100, early_stopping_rounds = 100)

xgb.importance(cols, model = m_xgb) %>% 
  xgb.plot.importance(top_n = 25)
```

Finally, we make predictions:
```{r xgb4, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
pred_xgb_tr <- predict(m_xgb, dtrain)
pred_xgb <- predict(m_xgb, dtest) 
sub <- "xgb_gs.csv"
# submit(pred_xgb)
pred_xgb[0:10]
rm(dtr, dtrain, dval, dtest, m_xgb); invisible(gc)

```
As it was stated earlier, **hits** and **pageviews** plays important roles in the XGB model. 

## Distributions of predictions
Let's compare predictions for the train set:
```{r pr_cmp0, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tibble(glmnet = pred_glm_tr, xgb = pred_xgb_tr, y = log1p(y)) %>% 
  mutate_all(funs(ifelse(. < 0, 0, .))) %>% 
  gather() %>% 
  ggplot(aes(x=value, fill=key)) +
  geom_histogram(binwidth = .05, alpha=.6, position="identity") +
  scale_y_continuous(labels = scales::comma) +
  theme_minimal() +
  facet_grid(key~.,  scales = "free", space = "fixed") +
  scale_x_continuous(limits = c(-0.05, 3))+
  labs(x = "predictions")
```

As we can see the distributions of the predictions are quite different. The XGB model 
tends to produce more narrow interval - closer to the true distribution.

```{r pr_cmp1, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
pred_avg <- log1p((expm1(pred_glm) + expm1(pred_xgb)) / 2)
sub <- "avg_gs.csv"
# submit(pred_xgb)
```
```{r pr_cmp2, result='asis', message=FALSE, warning=FALSE, echo=FALSE}
tibble(glmnet = pred_glm, xgb = pred_xgb, avg = pred_avg) %>% 
  mutate_all(funs(ifelse(. < 0, 0, .))) %>% 
  gather() %>% 
  ggplot(aes(x=value, fill=key)) +
  geom_histogram(binwidth = .05, alpha=.6, position="identity") +
  scale_y_continuous(labels = scales::comma) +
  theme_minimal() +
  facet_grid(key~.,  scales = "free", space = "fixed") +
  scale_x_continuous(limits = c(-0.05, 3))+
  labs(x = "predictions")
```

Distributions of the predictions for the test set differ much too, 
nevertheless after proper tuning of the models they can be useful for ensembling.

## Output/Write results to csv:

```{r pr_csv, result='asis', message=FALSE, warning=FALSE, echo=TRUE}
tr_dollar <- y/(10^6)
te_dollar <- exp(pred_avg)

tr_actl <- cbind(tr, tr_dollar)
te_pred <- cbind(te, te_dollar)

write_csv(tr_actl, "../output/tr_actl.csv")
write_csv(te_pred, "../output/te_pred.csv")

```


# The End by DD Consulting
